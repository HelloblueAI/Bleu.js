{
  "architecture": {
    "type": "transformer",
    "layers": 24,
    "attentionHeads": 16,
    "hiddenSize": 4096,
    "vocabularySize": 50000,
    "maxSequenceLength": 8192
  },
  "training": {
    "batchSize": 32,
    "learningRate": 1e-4,
    "epochs": 10,
    "warmupSteps": 1000
  },
  "inference": {
    "defaultMaxTokens": 2048,
    "defaultTemperature": 0.7,
    "defaultTopP": 0.9
  }
} 